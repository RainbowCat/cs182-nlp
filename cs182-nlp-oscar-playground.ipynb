{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cs182-nlp-oscar-playground.ipynb","provenance":[],"mount_file_id":"1InI9Hf486J0zv7dcSD2Y1rte5WFp1rBY","authorship_tag":"ABX9TyN4uK32H9zBdOK5O+5uQhqB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"e4727fc891464370986c73e83521ef1e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b986afeaaabb4a38bca561657042a219","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ac1deadf49b643c4857c0625fce1651f","IPY_MODEL_9775c2ed288149c2b83c0873b535f306"]}},"b986afeaaabb4a38bca561657042a219":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ac1deadf49b643c4857c0625fce1651f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5f2146fbcce448f1ab4a631bc871c59c","_dom_classes":[],"description":"  0%","_model_name":"FloatProgressModel","bar_style":"danger","max":8338,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":14,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_57c8f09a2f294daa99897280a2008364"}},"9775c2ed288149c2b83c0873b535f306":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d1e7873224164c89b90c954b16790eb4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 14/8338 [08:08&lt;78:56:39, 34.14s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_44b8f453655e4447a989f84750065eae"}},"5f2146fbcce448f1ab4a631bc871c59c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"57c8f09a2f294daa99897280a2008364":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d1e7873224164c89b90c954b16790eb4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"44b8f453655e4447a989f84750065eae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GYMfkkVXpizz","executionInfo":{"status":"ok","timestamp":1620706826331,"user_tz":420,"elapsed":25066,"user":{"displayName":"Oscar Chan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjinDwCEJSm9hr9rfF-zqndCjlev2TrfE6uRBtBcQ=s64","userId":"00500229992007072974"}},"outputId":"83b27eec-690f-43af-a908-f2dcaa3d79f7"},"source":["!pip install transformers\n","!pip install sentencepiece\n","!pip install segtok\n","!pip install vaderSentiment\n","!pip install nltk\n","!pip install huggingface_hub\n","!pip install pytorch-lightning"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.95)\n","Requirement already satisfied: segtok in /usr/local/lib/python3.7/dist-packages (1.5.10)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from segtok) (2019.12.20)\n","Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.7/dist-packages (3.3.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.7/dist-packages (0.0.8)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (2.23.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (4.41.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (3.0.12)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (3.10.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (2.10)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->huggingface_hub) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->huggingface_hub) (3.7.4.3)\n","Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.7/dist-packages (1.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (20.9)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.19.5)\n","Requirement already satisfied: PyYAML<=5.4.1,>=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (5.4.1)\n","Requirement already satisfied: torchmetrics>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (0.3.2)\n","Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (0.18.2)\n","Requirement already satisfied: tensorboard!=2.5.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2.4.1)\n","Requirement already satisfied: pyDeprecate==0.3.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (0.3.0)\n","Requirement already satisfied: fsspec[http]>=2021.4.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2021.4.0)\n","Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.8.1+cu101)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (4.41.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pytorch-lightning) (2.4.7)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.36.2)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (56.1.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.4.4)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.8.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.0.1)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.15.0)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.28.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.12.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.32.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.3.4)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.12.4)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (2.23.0)\n","Requirement already satisfied: aiohttp; extra == \"http\" in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>=2021.4.0->pytorch-lightning) (3.7.4.post0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->pytorch-lightning) (3.7.4.3)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.3.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (4.2.1)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (4.7.2)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.10.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=2021.4.0->pytorch-lightning) (1.6.3)\n","Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=2021.4.0->pytorch-lightning) (3.0.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=2021.4.0->pytorch-lightning) (20.3.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=2021.4.0->pytorch-lightning) (5.1.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.1.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.4.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.4.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-05-08T00:55:57.344686Z","start_time":"2021-05-08T00:55:57.327258Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"teIgRhnAcOfK","executionInfo":{"status":"ok","timestamp":1620706831057,"user_tz":420,"elapsed":29784,"user":{"displayName":"Oscar Chan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjinDwCEJSm9hr9rfF-zqndCjlev2TrfE6uRBtBcQ=s64","userId":"00500229992007072974"}},"outputId":"b3e85603-444b-4b47-abfd-f28f8d4e3b42"},"source":["import os\n","import sys\n","from pathlib import Path\n","\n","import json\n","import pandas as pd\n","import random\n","\n","import torch\n","from segtok import tokenizer\n","\n","import tqdm\n","\n","from multiprocessing import Pool\n","\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n","import tokenize\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import numpy as np\n","import torchvision\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","import copy\n","import nltk\n","nltk.download('punkt')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C7dLS11DSV3z","executionInfo":{"status":"ok","timestamp":1620706831058,"user_tz":420,"elapsed":29777,"user":{"displayName":"Oscar Chan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjinDwCEJSm9hr9rfF-zqndCjlev2TrfE6uRBtBcQ=s64","userId":"00500229992007072974"}},"outputId":"31146fda-233e-459d-e63f-026012005132"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"T6NrAREFVU6N","executionInfo":{"status":"ok","timestamp":1620706831060,"user_tz":420,"elapsed":29772,"user":{"displayName":"Oscar Chan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjinDwCEJSm9hr9rfF-zqndCjlev2TrfE6uRBtBcQ=s64","userId":"00500229992007072974"}}},"source":[""],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"fkMwDw2OS0Gk","executionInfo":{"status":"ok","timestamp":1620706848952,"user_tz":420,"elapsed":47655,"user":{"displayName":"Oscar Chan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjinDwCEJSm9hr9rfF-zqndCjlev2TrfE6uRBtBcQ=s64","userId":"00500229992007072974"}},"outputId":"ccf78b82-9bfc-498d-89ec-5aad28e135d5"},"source":["ROOT_FOLDER = Path(\"/content/drive/My Drive/cs182_final_project/cs182-nlp\")\n","DATA_FOLDER = ROOT_FOLDER / \"dataset\"\n","TORCH_CHECKPOINT_MODEL = ROOT_FOLDER / \"models\" / \"training_checkpoint_oscar_vader.pt\"\n","\n","input(\"Please check to make sure the above checkpoint directory is yours (Hit any key)\")"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Please check to make sure the above checkpoint directory is yours (Hit any key)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["''"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uQ6eZM_WTARe","executionInfo":{"status":"ok","timestamp":1620706850789,"user_tz":420,"elapsed":49478,"user":{"displayName":"Oscar Chan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjinDwCEJSm9hr9rfF-zqndCjlev2TrfE6uRBtBcQ=s64","userId":"00500229992007072974"}},"outputId":"007c203d-8212-4a2e-fc22-5fb3788fe7d0"},"source":["sys.path.append(str(ROOT_FOLDER))\n","\n","import data\n","import models"],"execution_count":6,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MhE0qDQrbmrF","executionInfo":{"status":"ok","timestamp":1620706850794,"user_tz":420,"elapsed":49477,"user":{"displayName":"Oscar Chan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjinDwCEJSm9hr9rfF-zqndCjlev2TrfE6uRBtBcQ=s64","userId":"00500229992007072974"}}},"source":["from argparse import Namespace\n","\n","args = Namespace(\n","    batch_size=32,\n","    epochs=10,\n","    max_len=128,\n","    max_len_vader=40,\n","    use_bert=False,\n","    use_cnn=True,\n","    use_vader=True,\n",")"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"j4qANCo5k-kz","executionInfo":{"status":"ok","timestamp":1620706850965,"user_tz":420,"elapsed":49643,"user":{"displayName":"Oscar Chan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjinDwCEJSm9hr9rfF-zqndCjlev2TrfE6uRBtBcQ=s64","userId":"00500229992007072974"}}},"source":["class LanguageModel(nn.Module):\n","    def __init__(self, vocab_size, rnn_size, vader_size, num_layers=1, dropout=0, use_vader=True):\n","        super().__init__()\n","        \n","        #################\n","        #    INPUT 1    #\n","        #################\n","        # Create an embedding layer, with 768 hidden layers\n","        self.xlnet = torch.hub.load('huggingface/pytorch-transformers', 'model', 'xlnet-base-cased')\n","        for param in self.xlnet.layer.parameters():\n","          param.requires_grad = False\n","        # Output: (vocab_size x 768), where 768 hidden layers of XLNet\n","\n","        # Coming in: torch.Size([BATCH_SIZE, vocab_size, 768])\n","        #   (XLNet has 768 hidden layers, https://huggingface.co/transformers/pretrained_models.html)\n","        conv2d_c_in = 1\n","        conv2d_c_out = 1\n","        conv2d_kernel_W = 5 # along Embedding Length\n","        conv2d_kernel_H = 5 # along Word Length\n","\n","        self.conv2D_layer = nn.Conv2d(conv2d_c_in, conv2d_c_out, (conv2d_kernel_H, conv2d_kernel_W))\n","        # Filter of (conv2d_kernel_H, conv2d_kernel_W), Cin = 1, Cout = 1\n","\n","        # Output:\n","        conv2d_out_Hout = vocab_size - ((conv2d_kernel_H - 1) // 2) * 2 # Vocab Size\n","        conv2d_out_Wout = 768 - ((conv2d_kernel_W - 1) // 2) * 2        # length\n","\n","        self.max_pool_2d = nn.MaxPool2d((conv2d_out_Hout, 1))\n","        max_pool_2d_out_height = conv2d_out_Hout // conv2d_out_Hout\n","        max_pool_2d_out_length = conv2d_out_Wout // 1\n","        #################\n","        #  INPUT 1 END  #\n","        #################\n","        \n","        #################\n","        #    INPUT 2    #\n","        #################\n","        self.lstm = None\n","        if use_vader:\n","          self.lstm = nn.LSTM(input_size=1, hidden_size=1, num_layers=num_layers, batch_first=True, dropout=dropout)\n","        else:\n","          vader_size = 0\n","        #################\n","        #  INPUT 2 END  #\n","        #################\n","\n","        self.dropout = nn.Dropout(dropout)\n","        # print(max_pool_2d_out_length + vader_size)\n","\n","        hidden_layer_dense = 100\n","\n","        self.dense = nn.Sequential(\n","                nn.Linear(max_pool_2d_out_length + vader_size, hidden_layer_dense),\n","                nn.ReLU()\n","            )\n","        self.output = nn.Linear(hidden_layer_dense, 6) # classify yelp_reviews into 5 ratings\n","    \n","    def forward_input_vectorized(self, x):\n","      xlnet_out = self.xlnet(x)\n","      xlnet_out_hidden = xlnet_out.last_hidden_state\n","      batches_len, word_len, embedding_len = xlnet_out_hidden.shape\n","      xlnet_out_hidden = xlnet_out_hidden.reshape(batches_len, 1, word_len, embedding_len)\n","      conv2d_out = self.conv2D_layer(xlnet_out_hidden)\n","      result = self.max_pool_2d(conv2d_out)\n","      # print(result.shape)\n","      result = result.squeeze(1).squeeze(1)\n","      return result\n","\n","    def forward_input_vader(self, x):\n","      batch_size, vader_len = x.shape\n","      # print(x.reshape(batch_size, vader_len, 1).shape)\n","      output, _ = self.lstm(x.reshape(batch_size, vader_len, 1))\n","      # print(output.shape)\n","      output = output.squeeze(2)\n","      return output\n","\n","    def predict(self, vectorized_words, vadar_sentiments):\n","        logits = self.forward(vectorized_words, vadar_sentiments)\n","        prediction = logits.argmax(dim=1, keepdim=False)\n","        return prediction\n","\n","    def forward(self, vectorized_words, vader):\n","        input1 = self.forward_input_vectorized(vectorized_words)\n","\n","        if self.lstm:\n","          input2 = self.forward_input_vader(vader)\n","          combined_input = (input1, input2)\n","        else:\n","          combined_input = (input1,) # Tuples need the stray comma\n","\n","        # print(input1.size(), input2.size())\n","\n","        combined_input = torch.cat(combined_input, dim=1)\n","\n","        lstm_drop = self.dropout(combined_input)\n","        logits = self.dense(lstm_drop)\n","        logits = self.output(logits)\n","        return logits\n","    \n","    def loss_fn(self, prediction, target):\n","      loss_criterion = nn.CrossEntropyLoss(reduction='none')\n","      return torch.mean(loss_criterion(prediction, target - 1))"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"j6xsUTpJMwpi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620706875637,"user_tz":420,"elapsed":74308,"user":{"displayName":"Oscar Chan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjinDwCEJSm9hr9rfF-zqndCjlev2TrfE6uRBtBcQ=s64","userId":"00500229992007072974"}},"outputId":"754b2a6b-2369-434c-ad87-69573f0e78ec"},"source":["import json\n","import pickle\n","import sys\n","\n","import nltk\n","import torch\n","import tqdm\n","\n","import data\n","import models\n","\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n","\n","MAX_LEN = 128\n","MAX_LEN_VADER = 40\n","BATCH_SIZE = 64\n","EPOCHS = 5\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model_params = torch.load(\n","    TORCH_CHECKPOINT_MODEL, map_location=device\n",")\n","\n","list_to_device = lambda th_obj: [tensor.to(device) for tensor in th_obj]\n","\n","model = LanguageModel(MAX_LEN, 256, MAX_LEN_VADER)\n","# vocab_size, rnn_size, vader_size, num_layers=1, dropout=0, use_vader=True)\n","\n","model.load_state_dict(model_params[\"model_state_dict\"])\n","model = model.to(device)\n","model.eval()\n","\n","analyzer = SentimentIntensityAnalyzer()\n","xlnet_tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'xlnet-base-cased')\n","\n","def predict_stars(texts):\n","    \"\"\"\n","    text - a SINGLE texts\n","    \"\"\"\n","    # This is where you call your model to get the number of stars output\n","    vectorized_list = []\n","    vadar_sentiments_list = []\n","    for text in texts:\n","      encodings = xlnet_tokenizer.encode_plus(\n","          text,\n","          add_special_tokens=True,\n","          max_length=MAX_LEN,\n","          return_token_type_ids=False,\n","          return_attention_mask=False,\n","          truncation=True,\n","          pad_to_max_length=False,\n","      )\n","      text_encoding = encodings.get(\"input_ids\", [])\n","      vectorized, _ = data.pad_sequence(text_encoding, 0, MAX_LEN)\n","      vectorized_list.append(vectorized)\n","\n","      sentence_list = nltk.tokenize.sent_tokenize(\n","          text\n","      )  # Text is one at a time anyway here\n","      review_sentiment_sentence = []\n","      for sentence in sentence_list:\n","          vs = analyzer.polarity_scores(sentence)\n","          review_sentiment_sentence.append(vs[\"compound\"])\n","      vadar_sentiments, _ = data.pad_sequence(review_sentiment_sentence, 0, MAX_LEN_VADER)\n","      vadar_sentiments_list.append(vadar_sentiments)\n","\n","    # Place the data as a batch, even if there is only 1\n","    vectorized = data.batch_to_torch_long(vectorized_list)\n","    vadar_sentiments = data.batch_to_torch_float(vadar_sentiments_list)\n","\n","    p = model.predict(vectorized, vadar_sentiments)\n","    return float(p.tolist())"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n","Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["e4727fc891464370986c73e83521ef1e","b986afeaaabb4a38bca561657042a219","ac1deadf49b643c4857c0625fce1651f","9775c2ed288149c2b83c0873b535f306","5f2146fbcce448f1ab4a631bc871c59c","57c8f09a2f294daa99897280a2008364","d1e7873224164c89b90c954b16790eb4","44b8f453655e4447a989f84750065eae"]},"id":"CWBvGwZST4Fe","executionInfo":{"status":"error","timestamp":1620708776286,"user_tz":420,"elapsed":493819,"user":{"displayName":"Oscar Chan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjinDwCEJSm9hr9rfF-zqndCjlev2TrfE6uRBtBcQ=s64","userId":"00500229992007072974"}},"outputId":"e4f20de0-267b-4052-dfbb-4c010903b899"},"source":["count = 0\n","total_time = 0\n","\n","if len(sys.argv) > 1:\n","    validation_file = \"/content/yelp_review_training_dataset.jsonl\"\n","    with open(\"output.jsonl\", \"w\") as fw:\n","        pandas_dataset = data.load_json(validation_file)\n","\n","        dataset_batch_cap = ( pandas_dataset.shape[0] // BATCH_SIZE ) + (1 if pandas_dataset.shape[0] % BATCH_SIZE > 0 else 0)\n","        \n","        t = tqdm.notebook.tqdm(range(0, dataset_batch_cap), initial = 0, total = dataset_batch_cap)\n","        \n","        for i in t:\n","          val_start_i = i*BATCH_SIZE\n","          val_end_i = (i+1)*BATCH_SIZE\n","          # print(val_start_i, val_end_i, indices.shape)\n","\n","          data_subset = pandas_dataset.iloc[val_start_i:val_end_i]\n","          \n","          # batch\n","          batch_val = data.format_reviews(args, datatable=data_subset)\n","\n","          \"\"\"\n","          return (\n","              torch.LongTensor(encoded_reviews),  # text\n","              torch.FloatTensor(review_sentiments),  # sentiments\n","              torch.LongTensor(reviews_to_process[\"stars\"].values),  # target\n","              torch.FloatTensor(encoded_reviews_mask),  # mask\n","          )\n","          \"\"\"\n","          start_time = time.time()\n","          (batch_input_val, batch_review_sentiment_val, batch_target_val, batch_target_mask_val) = batch_val\n","          # print(batch_input_val.shape, batch_review_sentiment_val.shape)\n","          (batch_input_val, batch_target_val) = list_to_device((batch_input_val, batch_target_val))\n","          batch_target_mask_val, batch_review_sentiment_val = list_to_device((batch_target_mask_val, batch_review_sentiment_val))\n","          end_time = time.time()\n","          print(\"process time\", end_time - start_time)\n","\n","          # forward pass\n","          start_time = time.time()\n","          prediction = model.predict(batch_input_val, batch_review_sentiment_val)\n","          end_time = time.time()\n","          total_time += end_time - start_time\n","\n","          # print(prediction)\n","          for i, pred_val in enumerate(prediction):\n","            pred_val = pred_val.item()\n","            count += 1\n","            fw.write(\n","                json.dumps(\n","                    {\n","                        \"review_id\": data_subset.iloc[i][\"review_id\"],\n","                        \"predicted_stars\": pred_val,\n","                    }\n","                )\n","                + \"\\n\"\n","            )\n","          \n","          print(count, total_time)\n","\n","\n","        # for i in t:\n","        # for line in fr:\n","        #     if i % 10 == 0:\n","        #       print(i)\n","        #     if i > 500:\n","        #       print(\"END\")\n","        #       break\n","        #     review = json.loads(line)\n","        #     fw.write(\n","        #         json.dumps(\n","        #             {\n","        #                 \"review_id\": review[\"review_id\"],\n","        #                 \"predicted_stars\": predict_stars(review[\"text\"]),\n","        #             }\n","        #         )\n","        #         + \"\\n\"\n","        #     )\n","        #     i += 1\n","    print(\"Output prediction file written\")\n","else:\n","    print(\"No validation file given\")"],"execution_count":19,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e4727fc891464370986c73e83521ef1e","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=8338.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","64it [00:00, 278.52it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["process time 6.532669067382812e-05\n","64 34.00685405731201\n"],"name":"stdout"},{"output_type":"stream","text":["Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n","64it [00:00, 358.37it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["process time 6.771087646484375e-05\n","128 67.64262819290161\n"],"name":"stdout"},{"output_type":"stream","text":["Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n","64it [00:00, 338.28it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["process time 7.653236389160156e-05\n","192 101.2265739440918\n"],"name":"stdout"},{"output_type":"stream","text":["Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n","64it [00:00, 305.37it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["process time 6.67572021484375e-05\n","256 134.92191576957703\n"],"name":"stdout"},{"output_type":"stream","text":["Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n","64it [00:00, 298.11it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["process time 8.058547973632812e-05\n","320 168.87473726272583\n"],"name":"stdout"},{"output_type":"stream","text":["Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n","64it [00:00, 349.33it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["process time 0.00012826919555664062\n","384 202.491459608078\n"],"name":"stdout"},{"output_type":"stream","text":["Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n","64it [00:00, 334.02it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["process time 6.747245788574219e-05\n","448 236.1170792579651\n"],"name":"stdout"},{"output_type":"stream","text":["Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n","64it [00:00, 338.02it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["process time 0.00010132789611816406\n","512 269.7074224948883\n"],"name":"stdout"},{"output_type":"stream","text":["Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n","64it [00:00, 355.76it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["process time 0.00012254714965820312\n","576 303.2699673175812\n"],"name":"stdout"},{"output_type":"stream","text":["Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n","64it [00:00, 296.93it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["process time 5.936622619628906e-05\n","640 336.8738431930542\n"],"name":"stdout"},{"output_type":"stream","text":["Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n","64it [00:00, 303.27it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["process time 7.534027099609375e-05\n","704 370.528146982193\n"],"name":"stdout"},{"output_type":"stream","text":["Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n","64it [00:00, 335.46it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["process time 7.653236389160156e-05\n","768 404.01006960868835\n"],"name":"stdout"},{"output_type":"stream","text":["Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n","64it [00:00, 279.04it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["process time 0.00015878677368164062\n","832 437.5544364452362\n"],"name":"stdout"},{"output_type":"stream","text":["Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n","64it [00:00, 326.87it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["process time 0.00013113021850585938\n","896 471.13776540756226\n"],"name":"stdout"},{"output_type":"stream","text":["Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n","64it [00:00, 387.02it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["process time 8.726119995117188e-05\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-e1f1f7c2dcb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m           \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m           \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m           \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_input_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_review_sentiment_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m           \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m           \u001b[0mtotal_time\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-6f431b2c3ce3>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, vectorized_words, vadar_sentiments)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorized_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvadar_sentiments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorized_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvadar_sentiments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-6f431b2c3ce3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, vectorized_words, vader)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorized_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0minput1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_input_vectorized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorized_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-6f431b2c3ce3>\u001b[0m in \u001b[0;36mforward_input_vectorized\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_input_vectorized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m       \u001b[0mxlnet_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m       \u001b[0mxlnet_out_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlnet_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m       \u001b[0mbatches_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlnet_out_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/xlnet/modeling_xlnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, mems, perm_mask, target_mapping, token_type_ids, input_mask, head_mask, inputs_embeds, use_mems, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1257\u001b[0m                 \u001b[0mtarget_mapping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_mapping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m                 \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1259\u001b[0;31m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m             )\n\u001b[1;32m   1261\u001b[0m             \u001b[0moutput_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/xlnet/modeling_xlnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, output_h, output_g, attn_mask_h, attn_mask_g, r, seg_mat, mems, target_mapping, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mtarget_mapping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_mapping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m         )\n\u001b[1;32m    528\u001b[0m         \u001b[0moutput_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/xlnet/modeling_xlnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, h, g, attn_mask_h, attn_mask_g, r, seg_mat, mems, target_mapping, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0mq_head_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ibh,hnd->ibnd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0mk_head_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ibh,hnd->ibnd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mv_head_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ibh,hnd->ibnd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;31m# positional heads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(equation, *operands)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_operands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"bRRcj3Q3T4zh","executionInfo":{"status":"aborted","timestamp":1620706966826,"user_tz":420,"elapsed":165485,"user":{"displayName":"Oscar Chan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjinDwCEJSm9hr9rfF-zqndCjlev2TrfE6uRBtBcQ=s64","userId":"00500229992007072974"}}},"source":[""],"execution_count":null,"outputs":[]}]}