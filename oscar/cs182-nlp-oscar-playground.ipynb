{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cs182-nlp-oscar-playground.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GYMfkkVXpizz","executionInfo":{"status":"ok","timestamp":1620760928864,"user_tz":420,"elapsed":39810,"user":{"displayName":"Chandana Bhimarao","photoUrl":"","userId":"17089847895379828126"}},"outputId":"f44c13f1-e98b-4e51-b4cc-5735e462f962"},"source":["!pip install transformers\n","!pip install sentencepiece\n","!pip install segtok\n","!pip install vaderSentiment\n","!pip install nltk\n","!pip install huggingface_hub\n","!pip install pytorch-lightning"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n","\u001b[K     |████████████████████████████████| 2.1MB 4.2MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 42.7MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 43.4MB/s \n","\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Installing collected packages: tokenizers, sacremoses, transformers\n","Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n","\u001b[K     |████████████████████████████████| 1.2MB 4.0MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.95\n","Collecting segtok\n","  Downloading https://files.pythonhosted.org/packages/41/08/582dab5f4b1d5ca23bc6927b4bb977c8ff7f3a87a3b98844ef833e2f5623/segtok-1.5.10.tar.gz\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from segtok) (2019.12.20)\n","Building wheels for collected packages: segtok\n","  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for segtok: filename=segtok-1.5.10-cp37-none-any.whl size=25019 sha256=ae3c0bf7785d0a04c356e929dad212092058604d693a22fff9f89c1db37401fe\n","  Stored in directory: /root/.cache/pip/wheels/b4/39/f6/9ca1c5cabde964d728023b5751c3a206a5c8cc40252321fb6b\n","Successfully built segtok\n","Installing collected packages: segtok\n","Successfully installed segtok-1.5.10\n","Collecting vaderSentiment\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/fc/310e16254683c1ed35eeb97386986d6c00bc29df17ce280aed64d55537e9/vaderSentiment-3.3.2-py2.py3-none-any.whl (125kB)\n","\u001b[K     |████████████████████████████████| 133kB 4.3MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n","Installing collected packages: vaderSentiment\n","Successfully installed vaderSentiment-3.3.2\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n","Collecting huggingface_hub\n","  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (4.41.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (3.0.12)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (3.10.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (2.23.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->huggingface_hub) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->huggingface_hub) (3.7.4.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (3.0.4)\n","Installing collected packages: huggingface-hub\n","Successfully installed huggingface-hub-0.0.8\n","Collecting pytorch-lightning\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/a1/a991780873b5fd760fb99dfda01916fe9e5b186f0ba70a120e6b4f79cfaa/pytorch_lightning-1.3.1-py3-none-any.whl (805kB)\n","\u001b[K     |████████████████████████████████| 808kB 5.7MB/s \n","\u001b[?25hCollecting pyDeprecate==0.3.0\n","  Downloading https://files.pythonhosted.org/packages/14/52/aa227a0884df71ed1957649085adf2b8bc2a1816d037c2f18b3078854516/pyDeprecate-0.3.0-py3-none-any.whl\n","Collecting future>=0.17.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n","\u001b[K     |████████████████████████████████| 829kB 9.1MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.8.1+cu101)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (4.41.1)\n","Requirement already satisfied: tensorboard!=2.5.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2.4.1)\n","Collecting fsspec[http]>=2021.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/91/2ef649137816850fa4f4c97c6f2eabb1a79bf0aa2c8ed198e387e373455e/fsspec-2021.4.0-py3-none-any.whl (108kB)\n","\u001b[K     |████████████████████████████████| 112kB 19.3MB/s \n","\u001b[?25hCollecting torchmetrics>=0.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/e8/513cd9d0b1c83dc14cd8f788d05cd6a34758d4fd7e4f9e5ecd5d7d599c95/torchmetrics-0.3.2-py3-none-any.whl (274kB)\n","\u001b[K     |████████████████████████████████| 276kB 16.5MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.19.5)\n","Collecting PyYAML<=5.4.1,>=5.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n","\u001b[K     |████████████████████████████████| 645kB 19.1MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (20.9)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->pytorch-lightning) (3.7.4.3)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.3.4)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.32.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.4.4)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.12.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (2.23.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.15.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.8.0)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.36.2)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.12.4)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.0.1)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.28.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (56.1.0)\n","Collecting aiohttp; extra == \"http\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/c0/5890b4c8b04a79b7360e8fe4490feb0bb3ab179743f199f0e6220cebd568/aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3MB)\n","\u001b[K     |████████████████████████████████| 1.3MB 23.5MB/s \n","\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pytorch-lightning) (2.4.7)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.10.1)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.3.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (2020.12.5)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (4.2.1)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (4.7.2)\n","Collecting yarl<2.0,>=1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/62/046834c5fc998c88ab2ef722f5d42122230a632212c8afa76418324f53ff/yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294kB)\n","\u001b[K     |████████████████████████████████| 296kB 34.1MB/s \n","\u001b[?25hCollecting multidict<7.0,>=4.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a6/4123b8165acbe773d1a8dc8e3f0d1edea16d29f7de018eda769abb56bd30/multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142kB)\n","\u001b[K     |████████████████████████████████| 143kB 43.1MB/s \n","\u001b[?25hCollecting async-timeout<4.0,>=3.0\n","  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=2021.4.0->pytorch-lightning) (20.3.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.4.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.1.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.4.8)\n","Building wheels for collected packages: future\n","  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for future: filename=future-0.18.2-cp37-none-any.whl size=491058 sha256=75e885b33ce2d36878647745fc4951ac0cf3b2738fa21fd00d9e3eb65c961a07\n","  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n","Successfully built future\n","Installing collected packages: pyDeprecate, future, multidict, yarl, async-timeout, aiohttp, fsspec, torchmetrics, PyYAML, pytorch-lightning\n","  Found existing installation: future 0.16.0\n","    Uninstalling future-0.16.0:\n","      Successfully uninstalled future-0.16.0\n","  Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed PyYAML-5.4.1 aiohttp-3.7.4.post0 async-timeout-3.0.1 fsspec-2021.4.0 future-0.18.2 multidict-5.1.0 pyDeprecate-0.3.0 pytorch-lightning-1.3.1 torchmetrics-0.3.2 yarl-1.6.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-05-08T00:55:57.344686Z","start_time":"2021-05-08T00:55:57.327258Z"},"id":"teIgRhnAcOfK"},"source":["import os\n","import sys\n","from pathlib import Path\n","\n","import json\n","import pandas as pd\n","import random\n","\n","import torch\n","from segtok import tokenizer\n","\n","import tqdm\n","\n","from multiprocessing import Pool\n","\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n","import tokenize\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import numpy as np\n","import torchvision\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","import copy\n","import nltk\n","nltk.download('punkt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C7dLS11DSV3z"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fkMwDw2OS0Gk"},"source":["from pathlib import Path\n","ROOT_FOLDER = Path(\"/content/drive/My Drive/cs182_final_project/cs182-nlp\")\n","DATA_FOLDER = ROOT_FOLDER / \"dataset\"\n","TORCH_CHECKPOINT_MODEL = ROOT_FOLDER / \"models\" / \"training_checkpoint_oscar_vader_sixthukn.pt\"\n","sys.path.append(str(ROOT_FOLDER))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uQ6eZM_WTARe"},"source":["sys.path.append(str(ROOT_FOLDER))\n","\n","import data\n","import models"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MhE0qDQrbmrF"},"source":["from argparse import Namespace\n","\n","args = Namespace(\n","    batch_size=32,\n","    epochs=10,\n","    max_len=128,\n","    max_len_vader=40,\n","    use_bert=False,\n","    use_cnn=True,\n","    use_vader=True,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j4qANCo5k-kz"},"source":["class LanguageModel(nn.Module):\n","    def __init__(self, vocab_size, rnn_size, vader_size, num_layers=1, dropout=0, use_vader=True):\n","        super().__init__()\n","        \n","        #################\n","        #    INPUT 1    #\n","        #################\n","        # Create an embedding layer, with 768 hidden layers\n","        self.xlnet = torch.hub.load('huggingface/pytorch-transformers', 'model', 'xlnet-base-cased')\n","        for param in self.xlnet.layer.parameters():\n","          param.requires_grad = False\n","        # Output: (vocab_size x 768), where 768 hidden layers of XLNet\n","\n","        # Coming in: torch.Size([BATCH_SIZE, vocab_size, 768])\n","        #   (XLNet has 768 hidden layers, https://huggingface.co/transformers/pretrained_models.html)\n","        conv2d_c_in = 1\n","        conv2d_c_out = 1\n","        conv2d_kernel_W = 5 # along Embedding Length\n","        conv2d_kernel_H = 5 # along Word Length\n","\n","        self.conv2D_layer = nn.Conv2d(conv2d_c_in, conv2d_c_out, (conv2d_kernel_H, conv2d_kernel_W))\n","        # Filter of (conv2d_kernel_H, conv2d_kernel_W), Cin = 1, Cout = 1\n","\n","        # Output:\n","        conv2d_out_Hout = vocab_size - ((conv2d_kernel_H - 1) // 2) * 2 # Vocab Size\n","        conv2d_out_Wout = 768 - ((conv2d_kernel_W - 1) // 2) * 2        # length\n","\n","        self.max_pool_2d = nn.MaxPool2d((conv2d_out_Hout, 1))\n","        max_pool_2d_out_height = conv2d_out_Hout // conv2d_out_Hout\n","        max_pool_2d_out_length = conv2d_out_Wout // 1\n","        #################\n","        #  INPUT 1 END  #\n","        #################\n","        \n","        #################\n","        #    INPUT 2    #\n","        #################\n","        self.lstm = None\n","        if use_vader:\n","          self.lstm = nn.LSTM(input_size=1, hidden_size=1, num_layers=num_layers, batch_first=True, dropout=dropout)\n","        else:\n","          vader_size = 0\n","        #################\n","        #  INPUT 2 END  #\n","        #################\n","\n","        self.dropout = nn.Dropout(dropout)\n","        # print(max_pool_2d_out_length + vader_size)\n","\n","        hidden_layer_dense = 100\n","\n","        self.dense = nn.Sequential(\n","                nn.Linear(max_pool_2d_out_length + vader_size, hidden_layer_dense),\n","                nn.ReLU()\n","            )\n","        self.output = nn.Linear(hidden_layer_dense, 6) # classify yelp_reviews into 5 ratings\n","    \n","    xlnet_timing = 0\n","    def forward_input_vectorized(self, x):\n","      start_time = time.time()\n","      xlnet_out = self.xlnet(x)\n","      end_time = time.time()\n","\n","      self.xlnet_timing += end_time - start_time\n","\n","      xlnet_out_hidden = xlnet_out.last_hidden_state\n","      batches_len, word_len, embedding_len = xlnet_out_hidden.shape\n","      xlnet_out_hidden = xlnet_out_hidden.reshape(batches_len, 1, word_len, embedding_len)\n","      conv2d_out = self.conv2D_layer(xlnet_out_hidden)\n","      result = self.max_pool_2d(conv2d_out)\n","      # print(result.shape)\n","      result = result.squeeze(1).squeeze(1)\n","      return result\n","\n","    def forward_input_vader(self, x):\n","      batch_size, vader_len = x.shape\n","      # print(x.reshape(batch_size, vader_len, 1).shape)\n","      output, _ = self.lstm(x.reshape(batch_size, vader_len, 1))\n","      # print(output.shape)\n","      output = output.squeeze(2)\n","      return output\n","\n","    def predict(self, vectorized_words, vadar_sentiments):\n","        logits = self.forward(vectorized_words, vadar_sentiments)\n","        prediction = logits.argmax(dim=1, keepdim=False)\n","        return prediction\n","\n","    total_time_concat = 0\n","    def forward(self, vectorized_words, vader):\n","        input1 = self.forward_input_vectorized(vectorized_words)\n","\n","        if False and self.lstm:\n","          input2 = self.forward_input_vader(vader)\n","          combined_input = (input1, input2)\n","        else:\n","          input2 = torch.zeros(input1.size()[0], 40)\n","          combined_input = (input1, input2) # Tuples need the stray comma\n","\n","        # print(input1.size(), input2.size())\n","\n","        start_time = time.time()\n","        combined_input = torch.cat(combined_input, dim=1)\n","        end_time = time.time()\n","\n","        self.total_time_concat += end_time - start_time\n","\n","        lstm_drop = self.dropout(combined_input)\n","        logits = self.dense(lstm_drop)\n","        logits = self.output(logits)\n","        return logits\n","    \n","    def loss_fn(self, prediction, target):\n","      loss_criterion = nn.CrossEntropyLoss(reduction='none')\n","      return torch.mean(loss_criterion(prediction, target - 1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4UWxYd1T0Mk_"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j6xsUTpJMwpi"},"source":["import json\n","import pickle\n","import sys\n","\n","import nltk\n","import torch\n","import tqdm\n","\n","import data\n","import models\n","\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n","\n","MAX_LEN = 128\n","MAX_LEN_VADER = 40\n","BATCH_SIZE = 64\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model_params = torch.load(\n","    TORCH_CHECKPOINT_MODEL, map_location=device\n",")\n","\n","list_to_device = lambda th_obj: [tensor.to(device) for tensor in th_obj]\n","\n","model = LanguageModel(MAX_LEN, 256, MAX_LEN_VADER)\n","# vocab_size, rnn_size, vader_size, num_layers=1, dropout=0, use_vader=True)\n","\n","model.load_state_dict(model_params[\"model_state_dict\"])\n","model = model.to(device)\n","model.eval()\n","\n","analyzer = SentimentIntensityAnalyzer()\n","xlnet_tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'xlnet-base-cased')\n","\n","def predict_stars(texts):\n","    \"\"\"\n","    text - a SINGLE texts\n","    \"\"\"\n","    # This is where you call your model to get the number of stars output\n","    vectorized_list = []\n","    vadar_sentiments_list = []\n","    for text in texts:\n","      encodings = xlnet_tokenizer.encode_plus(\n","          text,\n","          add_special_tokens=True,\n","          max_length=MAX_LEN,\n","          return_token_type_ids=False,\n","          return_attention_mask=False,\n","          truncation=True,\n","          pad_to_max_length=False,\n","      )\n","      text_encoding = encodings.get(\"input_ids\", [])\n","      vectorized, _ = data.pad_sequence(text_encoding, 0, MAX_LEN)\n","      vectorized_list.append(vectorized)\n","\n","      sentence_list = nltk.tokenize.sent_tokenize(\n","          text\n","      )  # Text is one at a time anyway here\n","      review_sentiment_sentence = []\n","      for sentence in sentence_list:\n","          vs = analyzer.polarity_scores(sentence)\n","          review_sentiment_sentence.append(vs[\"compound\"])\n","      vadar_sentiments, _ = data.pad_sequence(review_sentiment_sentence, 0, MAX_LEN_VADER)\n","      vadar_sentiments_list.append(vadar_sentiments)\n","\n","    # Place the data as a batch, even if there is only 1\n","    vectorized = data.batch_to_torch_long(vectorized_list)\n","    vadar_sentiments = data.batch_to_torch_float(vadar_sentiments_list)\n","\n","    p = model.predict(vectorized, vadar_sentiments)\n","    return p.tolist()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CWBvGwZST4Fe"},"source":["count = 0\n","total_time = 0\n","\n","model.eval()\n","\n","model.total_time_concat = 0\n","model.xlnet_timing = 0\n","\n","if len(sys.argv) > 1:\n","    validation_file = \"/content/yelp_review_training_dataset.jsonl\"\n","    with open(\"output.jsonl\", \"w\") as fw:\n","        pandas_dataset = data.load_json(validation_file)\n","\n","        dataset_batch_cap = ( pandas_dataset.shape[0] // BATCH_SIZE ) + (1 if pandas_dataset.shape[0] % BATCH_SIZE > 0 else 0)\n","        \n","        t = tqdm.notebook.tqdm(range(0, dataset_batch_cap), initial = 0, total = dataset_batch_cap)\n","        \n","        for i in t:\n","          val_start_i = i*BATCH_SIZE\n","          val_end_i = (i+1)*BATCH_SIZE\n","          # print(val_start_i, val_end_i, indices.shape)\n","\n","          data_subset = pandas_dataset.iloc[val_start_i:val_end_i]\n","          \n","          # batch\n","          batch_val = data.format_reviews(args, datatable=data_subset)\n","\n","          \"\"\"\n","          return (\n","              torch.LongTensor(encoded_reviews),  # text\n","              torch.FloatTensor(review_sentiments),  # sentiments\n","              torch.LongTensor(reviews_to_process[\"stars\"].values),  # target\n","              torch.FloatTensor(encoded_reviews_mask),  # mask\n","          )\n","          \"\"\"\n","          start_time = time.time()\n","          (batch_input_val, batch_review_sentiment_val, batch_target_val, batch_target_mask_val) = batch_val\n","          # print(batch_input_val.shape, batch_review_sentiment_val.shape)\n","          (batch_input_val, batch_target_val) = list_to_device((batch_input_val, batch_target_val))\n","          batch_target_mask_val, batch_review_sentiment_val = list_to_device((batch_target_mask_val, batch_review_sentiment_val))\n","          end_time = time.time()\n","          print(\"process time\", end_time - start_time)\n","\n","          # forward pass\n","          start_time = time.time()\n","          prediction = model.predict(batch_input_val, batch_review_sentiment_val)\n","          end_time = time.time()\n","          total_time += end_time - start_time\n","\n","          # print(prediction)\n","          for i, pred_val in enumerate(prediction):\n","            pred_val = pred_val.item()\n","            count += 1\n","            fw.write(\n","                json.dumps(\n","                    {\n","                        \"review_id\": data_subset.iloc[i][\"review_id\"],\n","                        \"predicted_stars\": float(pred_val),\n","                    }\n","                )\n","                + \"\\n\"\n","            )\n","          \n","          print(count, total_time, model.total_time_concat, total_time - model.total_time_concat)\n","          print(\"xlnet_timing\", model.xlnet_timing)\n","\n","    print(\"Output prediction file written\")\n","else:\n","    print(\"No validation file given\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bRRcj3Q3T4zh"},"source":[""],"execution_count":null,"outputs":[]}]}