{"cells":[{"cell_type":"markdown","metadata":{"id":"XghaLOekxmy3"},"source":["# Global Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"GYMfkkVXpizz"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n","\u001b[K     |████████████████████████████████| 2.1MB 8.1MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: importlib-metadata; python_version \u003c \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n","Requirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 46.5MB/s \n","\u001b[?25hCollecting tokenizers\u003c0.11,\u003e=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 47.5MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: pyparsing\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-\u003etransformers) (2.4.7)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version \u003c \"3.8\"-\u003etransformers) (3.4.1)\n","Requirement already satisfied: typing-extensions\u003e=3.6.4; python_version \u003c \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version \u003c \"3.8\"-\u003etransformers) (3.7.4.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers) (7.1.2)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (1.24.3)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (2020.12.5)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (2.10)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (3.0.4)\n","Installing collected packages: sacremoses, tokenizers, transformers\n","Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n","\u001b[K     |████████████████████████████████| 1.2MB 7.8MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.95\n","Collecting segtok\n","  Downloading https://files.pythonhosted.org/packages/41/08/582dab5f4b1d5ca23bc6927b4bb977c8ff7f3a87a3b98844ef833e2f5623/segtok-1.5.10.tar.gz\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from segtok) (2019.12.20)\n","Building wheels for collected packages: segtok\n","  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for segtok: filename=segtok-1.5.10-cp37-none-any.whl size=25019 sha256=1e9d8db047fffea391e836cb61d44c97c32ee9d493a12de08937fb44caf8a400\n","  Stored in directory: /root/.cache/pip/wheels/b4/39/f6/9ca1c5cabde964d728023b5751c3a206a5c8cc40252321fb6b\n","Successfully built segtok\n","Installing collected packages: segtok\n","Successfully installed segtok-1.5.10\n","Collecting vaderSentiment\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/fc/310e16254683c1ed35eeb97386986d6c00bc29df17ce280aed64d55537e9/vaderSentiment-3.3.2-py2.py3-none-any.whl (125kB)\n","\u001b[K     |████████████████████████████████| 133kB 7.8MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003evaderSentiment) (2.10)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003evaderSentiment) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-\u003evaderSentiment) (1.24.3)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003evaderSentiment) (2020.12.5)\n","Installing collected packages: vaderSentiment\n","Successfully installed vaderSentiment-3.3.2\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n","Collecting huggingface_hub\n","  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n","Requirement already satisfied: importlib-metadata; python_version \u003c \"3.8\" in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (3.10.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (3.0.12)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (4.41.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (2.23.0)\n","Requirement already satisfied: typing-extensions\u003e=3.6.4; python_version \u003c \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version \u003c \"3.8\"-\u003ehuggingface_hub) (3.7.4.3)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version \u003c \"3.8\"-\u003ehuggingface_hub) (3.4.1)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003ehuggingface_hub) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-\u003ehuggingface_hub) (1.24.3)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003ehuggingface_hub) (2020.12.5)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003ehuggingface_hub) (3.0.4)\n","Installing collected packages: huggingface-hub\n","Successfully installed huggingface-hub-0.0.8\n"]}],"source":["!pip install transformers\n","!pip install sentencepiece\n","!pip install segtok\n","!pip install vaderSentiment\n","!pip install nltk\n","!pip install huggingface_hub"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"teIgRhnAcOfK"},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","import sys\n","from pathlib import Path\n","\n","import json\n","import pandas as pd\n","import random\n","\n","import torch\n","from segtok import tokenizer\n","\n","import tqdm\n","\n","from multiprocessing import Pool\n","\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n","import tokenize\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import numpy as np\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","import copy\n","import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"C7dLS11DSV3z"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lhLSVms9xmiZ"},"outputs":[],"source":["ROOT_FOLDER = Path(\"/content/drive/My Drive/cs182_final_project/cs182-nlp\")\n","DATA_FOLDER = ROOT_FOLDER / \"dataset\"\n","TORCH_CHECKPOINT_MODEL = ROOT_FOLDER / \"models\" / \"training_checkpoint_oscar_tallercnn_vadar_5-class_5-11-2021.pt\"\n","\n","input(\"Please check to make sure the above checkpoint directory is yours (Hit any key)\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"T6NrAREFVU6N"},"outputs":[],"source":["sys.path.append(ROOT_FOLDER)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sod7rjQhEZ-f"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GM5I19eZPHOM"},"outputs":[],"source":["list_to_device = lambda th_obj: [tensor.to(device) for tensor in th_obj]"]},{"cell_type":"markdown","metadata":{"id":"PdLbOwAcfqBc"},"source":["# Model Params"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Iy1kMEptOpzJ"},"outputs":[],"source":["MAX_LEN = 128\n","MAX_LEN_VADER = 40\n","BATCH_SIZE = 32\n","EPOCHS = 3\n","USE_VADER = True\n","\n","# Higher bound settings: MAX_LEN = 256 and BATCH_SIZE = 16"]},{"cell_type":"markdown","metadata":{"id":"hGxTNUc8qqjq"},"source":["#Data Preprocessing Functions"]},{"cell_type":"markdown","metadata":{"id":"J-HKrL4yEwnO"},"source":["## load data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jZL9NI9busFJ"},"outputs":[],"source":["def load_json(file_path, filter_function=lambda x: True):\n","  \"\"\"\n","  file_path - full path of the file to read from\n","  filter_function - a data selection function, returns True to ADD a data point\n","  \"\"\"\n","  result = []\n","\n","  try:\n","    with open(file_path, \"r\") as f:\n","      for line in f:\n","        json_line = json.loads(line)\n","        if not filter_function(json_line):\n","          # Disallow via opposite of allow\n","          continue\n","        result.append(json_line) # each line is one data point dictionary\n","    return pd.DataFrame.from_records(result)\n","    # return result\n","\n","  except IOError:\n","    print(f\"cannot open {file_path}\")\n","    return None"]},{"cell_type":"markdown","metadata":{"id":"at2flD8Wg1nV"},"source":["## data formatting"]},{"cell_type":"markdown","metadata":{"id":"M5YKeRALgX4-"},"source":["### tokenize"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oh2twiqQfNBd"},"outputs":[],"source":["def tokenize(data):\n","  \"\"\"\n","  data - an iterable of sentences\n","  \"\"\"\n","  token_set = set()\n","  i = 0\n","  for sentences in data:\n","    if i % 1000 == 0:\n","      print(i, end=\", \" if i % 15000 != 0 else \"\\n\")\n","    tokenized = nltk.word_tokenize(sentences.lower())\n","    for token in tokenized:\n","      token_set.add(token)\n","    i += 1\n","  return token_set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t-lRyY58Fdzo"},"outputs":[],"source":["def tokenize_review(tokenizer, review_text):\n","  encodings = tokenizer.encode_plus(review_text, add_special_tokens=True,\n","                                    max_length=MAX_LEN,\n","                                    return_token_type_ids=False,\n","                                    return_attention_mask=False,\n","                                    truncation=True,\n","                                    pad_to_max_length=False)\n","  return encodings.get(\"input_ids\", [])"]},{"cell_type":"markdown","metadata":{"id":"RyPCOdSzgaYU"},"source":["### padding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jqr1GVExfo4J"},"outputs":[],"source":["def pad_sequence(numerized, pad_index, to_length, beginning=True):\n","    pad = numerized[:to_length]\n","    if beginning:\n","      padded = [pad_index] * (to_length - len(pad)) + pad\n","    else:\n","      padded = pad + [pad_index] * (to_length - len(pad))\n","    mask = [w != pad_index for w in padded]\n","    return padded, mask"]},{"cell_type":"markdown","metadata":{"id":"v419A-McO76J"},"source":["### batching"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W9eCbz-mO-GD"},"outputs":[],"source":["def batch_to_torch_long(*batches):\n","  if len(batches) == 1:\n","    return torch.LongTensor(batches[0])\n","  return [torch.LongTensor(batch) for batch in batches]\n","\n","def batch_to_torch_float(*batches):\n","  if len(batches) == 1:\n","    return torch.FloatTensor(batches[0])\n","  return [torch.FloatTensor(batch) for batch in batches]"]},{"cell_type":"markdown","metadata":{"id":"LBNc8FdKgf0a"},"source":["### full data format"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uaVjAekFN22E"},"outputs":[],"source":["analyzer = SentimentIntensityAnalyzer()\n","\n","def format_reviews(tokenizer, datatable, indices=None, task_bar=False, review_sentiment_dict=None):\n","  encoded_reviews = []\n","  encoded_reviews_mask = []\n","  review_sentiment = []\n","  reviews_to_process = datatable[[\"review_id\", \"text\", \"stars\"]]\n","  # display(reviews_to_process)\n","  if indices is not None:\n","    reviews_to_process = reviews_to_process.iloc[indices]\n","  \n","  review_iterator = reviews_to_process.iterrows()\n","  if task_bar:\n","    review_iterator = tqdm.notebook.tqdm(reviews_to_process.iterrows(), total=reviews_to_process.shape[0])\n","\n","  for i, review in review_iterator:\n","    # Tokenize by TOKENIZER\n","    review_text = review[\"text\"]\n","    numerized = tokenize_review(tokenizer, review_text)\n","    padded, mask = pad_sequence(numerized, 0, MAX_LEN)\n","    encoded_reviews.append(padded)\n","    encoded_reviews_mask.append(mask)\n","    # VADER\n","    if review_sentiment_dict is None:\n","      sentence_list = nltk.tokenize.sent_tokenize(review_text)\n","      review_sentiment_sentence = []\n","      for sentence in sentence_list:\n","          vs = analyzer.polarity_scores(sentence)\n","          review_sentiment_sentence.append(vs[\"compound\"])\n","      padded, _ = pad_sequence(review_sentiment_sentence, 0, MAX_LEN_VADER)\n","      review_sentiment.append(padded)\n","    else:\n","      if review[\"review_id\"] in review_sentiment_dict:\n","        review_sentiment.append(review_sentiment_dict[review[\"review_id\"]])\n","    \n","  torch_encoded_reviews, torch_encoded_reviews_target = \\\n","                    batch_to_torch_long(encoded_reviews, reviews_to_process[\"stars\"].values)\n","  torch_encoded_reviews_mask, torch_review_sentiment = batch_to_torch_float(encoded_reviews_mask, review_sentiment)\n","  return torch_encoded_reviews, torch_encoded_reviews_target, torch_review_sentiment, torch_encoded_reviews_mask"]},{"cell_type":"markdown","metadata":{"id":"U9K3qA8VmLDZ"},"source":["### split dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gduWzCvelhax"},"outputs":[],"source":["# https://stackoverflow.com/questions/38250710/how-to-split-data-into-3-sets-train-validation-and-test\n","def train_validate_test_split(df, train_percent=.6, validate_percent=.2, seed=0):\n","    np.random.seed(seed)\n","    perm = np.random.permutation(df.index)\n","    # m = df.size\n","    m = len(df.index)\n","\n","    train_end = int(train_percent * m)\n","    validate_end = int(validate_percent * m) + train_end\n","\n","    train = df.iloc[perm[:train_end]]\n","    validate = df.iloc[perm[train_end:validate_end]]\n","    test = df.iloc[perm[validate_end:]]\n","\n","    assert train.size + validate.size + test.size == df.size\n","\n","    return train, validate, test"]},{"cell_type":"markdown","metadata":{"id":"EEoBjxheh-gb"},"source":["# Data Preprocessing Code"]},{"cell_type":"markdown","metadata":{"id":"RruLo4HwiChr"},"source":["## load data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wzS2NTJqc79K"},"outputs":[],"source":["# load yelp data\n","yelp_reviews = load_json(DATA_FOLDER / \"yelp_review_training_dataset.jsonl\")\n","print(\"loaded\", len(yelp_reviews.index), \"data points\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q2IzDWFoEPcZ"},"outputs":[],"source":["display(yelp_reviews)"]},{"cell_type":"markdown","metadata":{"id":"cd2SXk_agslk"},"source":["## format + split data into train, val, and test sets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PGEdPRpbhb9z"},"outputs":[],"source":["xlnet_tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'xlnet-base-cased')\n","# tokenize_review(xlnet_tokenizer, \"I love this grub!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ob81x3cLiXAv"},"outputs":[],"source":["# train 75% | validation 15% | test 10%\n","train_ratio = .50\n","validate_ratio = .40\n","test_ratio = .10\n","assert train_ratio + validate_ratio + test_ratio == 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q6xjrhkzjUcm"},"outputs":[],"source":["train_reviews, validate_reviews, test_reviews = train_validate_test_split(yelp_reviews, train_ratio, validate_ratio)\n","# train_reviews_df, val_reviews_df, test_reviews_df = train_validate_test_split(yelp_reviews, train_ratio, validate_ratio)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gdzwOc54jUkf"},"outputs":[],"source":["# train_reviews, train_reviews_target, train_reviews_mask = format_reviews(xlnet_tokenizer, train_reviews_df)\n","# validate_reviews, test_reviews_target, validate_reviews_mask = format_reviews(xlnet_tokenizer, validate_reviews_df)\n","# test_reviews, test_reviews_target, _ = format_reviews(xlnet_tokenizer, test_reviews_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"niyVE-CHsO2r"},"outputs":[],"source":["print(len(train_reviews.index), \"yelp reviews for training\")\n","train_reviews"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"84p74Bn8sRb8"},"outputs":[],"source":["print(len(validate_reviews.index), \"yelp reviews for validation\")\n","validate_reviews"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ox01raxTsnyC"},"outputs":[],"source":["print(len(test_reviews.index), \"yelp reviews for testing\")\n","test_reviews"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pDo9hqAyxjC4"},"outputs":[],"source":["review_sentiment_dict = {}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fck2VkAgfP2n"},"outputs":[],"source":["# Create dictionary of all the reviews' Vader temporarily\n","\n","if USE_VADER:\n","  review_iterator = tqdm.notebook.tqdm(yelp_reviews.iterrows(), total=yelp_reviews.shape[0])\n","\n","  for i, review in review_iterator:\n","    # Tokenize by TOKENIZER\n","    review_text = review[\"text\"]\n","    # VADER\n","    sentence_list = nltk.tokenize.sent_tokenize(review_text)\n","    review_sentiment_sentence = []\n","    for sentence in sentence_list:\n","        vs = analyzer.polarity_scores(sentence)\n","        review_sentiment_sentence.append(vs[\"compound\"])\n","    padded, _ = pad_sequence(review_sentiment_sentence, 0, MAX_LEN_VADER)\n","    review_sentiment_dict[review[\"review_id\"]] = padded\n","    if len(review_sentiment_dict) \u003c 20:\n","      print(len(review_sentiment_dict), review_sentiment_dict[review[\"review_id\"]])\n"]},{"cell_type":"markdown","metadata":{"id":"_whw8dsGflSM"},"source":["#Model"]},{"cell_type":"markdown","metadata":{"id":"1lIKIVDkqkQw"},"source":["## model construction"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"4yhEy06QGZOY"},"outputs":[],"source":["class LanguageModel(nn.Module):\n","    def __init__(self, vocab_size, rnn_size, vader_size, num_layers=1, dropout=0, use_vader=USE_VADER):\n","        super().__init__()\n","        \n","        #################\n","        #    INPUT 1    #\n","        #################\n","        # Create an embedding layer, with 768 hidden layers\n","        self.xlnet = torch.hub.load('huggingface/pytorch-transformers', 'model', 'xlnet-base-cased')\n","        for param in self.xlnet.layer.parameters():\n","          param.requires_grad = False\n","        # Output: (vocab_size x 768), where 768 hidden layers of XLNet\n","\n","        # Coming in: torch.Size([BATCH_SIZE, vocab_size, 768])\n","        #   (XLNet has 768 hidden layers, https://huggingface.co/transformers/pretrained_models.html)\n","        conv2d_c_in = 1\n","        conv2d_c_out = 100\n","        conv2d_kernel_H = 768 # along Embedding Length\n","        conv2d_kernel_W = 5 # along Word Length\n","\n","        self.conv2D_layer = nn.Conv2d(conv2d_c_in, conv2d_c_out, (conv2d_kernel_W, conv2d_kernel_H))\n","        # Filter of (conv2d_kernel_W, conv2d_kernel_H), Cin = 1, Cout = 1\n","\n","        # conv2d_out torch.Size([32, 100, 124, 1])\n","\n","        # Output:\n","        conv2d_out_Wout = 1 + (vocab_size - conv2d_kernel_W) # Vocab Size\n","        conv2d_out_Hout = 1 + (768 - conv2d_kernel_H)       # length\n","\n","        self.max_pool_2d = nn.MaxPool2d((conv2d_out_Wout, 1))\n","        max_pool_2d_out_length = conv2d_out_Wout // conv2d_out_Wout\n","        max_pool_2d_out_height = conv2d_out_Hout // 1\n","        #################\n","        #  INPUT 1 END  #\n","        #################\n","        \n","        #################\n","        #    INPUT 2    #\n","        #################\n","        self.lstm = None\n","        if use_vader:\n","          self.lstm = nn.LSTM(input_size=1, hidden_size=1, num_layers=num_layers, batch_first=True, dropout=dropout)\n","        else:\n","          vader_size = 0\n","        #################\n","        #  INPUT 2 END  #\n","        #################\n","\n","        self.dropout = nn.Dropout(dropout)\n","        # print(max_pool_2d_out_height, max_pool_2d_out_length, vader_size)\n","\n","        hidden_layer_dense = 100\n","\n","        self.dense = nn.Sequential(\n","                nn.Linear(100 + vader_size, hidden_layer_dense),\n","                nn.ReLU()\n","            )\n","        self.output = nn.Linear(hidden_layer_dense, 5) # classify yelp_reviews into 5 ratings\n","    \n","    def forward_input_vectorized(self, x):\n","      xlnet_out = self.xlnet(x)\n","      xlnet_out_hidden = xlnet_out.last_hidden_state\n","      batches_len, word_len, embedding_len = xlnet_out_hidden.shape\n","      xlnet_out_hidden = xlnet_out_hidden.reshape(batches_len, 1, word_len, embedding_len)\n","      conv2d_out = self.conv2D_layer(xlnet_out_hidden)\n","      result = self.max_pool_2d(conv2d_out)\n","      result = result.squeeze(2).squeeze(2)\n","      return result\n","\n","    def forward_input_vader(self, x):\n","      batch_size, vader_len = x.shape\n","      # print(x.reshape(batch_size, vader_len, 1).shape)\n","      output, _ = self.lstm(x.reshape(batch_size, vader_len, 1))\n","      # print(output.shape)\n","      output = output.squeeze(2)\n","      return output\n","\n","    def forward(self, vectorized_words, vader):\n","        input1 = self.forward_input_vectorized(vectorized_words)\n","\n","        if self.lstm:\n","          input2 = self.forward_input_vader(vader)\n","          combined_input = (input1, input2)\n","        else:\n","          combined_input = (input1,) # Tuples need the stray comma\n","\n","        # print(input1.size(), input2.size())\n","\n","        combined_input = torch.cat(combined_input, dim=1)\n","\n","        lstm_drop = self.dropout(combined_input)\n","        logits = self.dense(lstm_drop)\n","        logits = self.output(logits)\n","        return logits\n","    \n","    def loss_fn(self, prediction, target):\n","      loss_criterion = nn.CrossEntropyLoss(reduction='none')\n","      return torch.mean(loss_criterion(prediction, target - 1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-gjNUgbnt5E_"},"outputs":[],"source":["model = LanguageModel(vocab_size=MAX_LEN, rnn_size=256, vader_size=MAX_LEN_VADER)"]},{"cell_type":"markdown","metadata":{"id":"pqmGvbHYGRfZ"},"source":["## train the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G3cG2Ur_k_f1"},"outputs":[],"source":["# num_of_validaion_set = 20 #len(validate_reviews)\n","\n","# batch_val = format_reviews(xlnet_tokenizer, validate_reviews, range(num_of_validaion_set), review_sentiment_dict=review_sentiment_dict) # This cell may take a while\n","\n","# (batch_input_val, batch_target_val, batch_review_sentiment_val, batch_target_mask_val) = batch_val"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J6nHqDpEjnTI"},"outputs":[],"source":["def run_validation(model, use_all=False, mode=\"val\"):\n","  reviews_dataset = None\n","  if mode == \"val\":\n","    print(\"Running Validation\")\n","    mode = \"Validation\"\n","    reviews_dataset = validate_reviews\n","  elif mode == \"test\":\n","    print(\"Running Testing\")\n","    mode = \"Test\"\n","    reviews_dataset = test_reviews\n","  else:\n","    assert False, \"Invalid mode\"\n","  num_of_review_set = len(reviews_dataset) if use_all else 1000\n","  indices = np.random.permutation(len(reviews_dataset))\n","  t = tqdm.notebook.tqdm(range(0, ( num_of_review_set // BATCH_SIZE) + ( 1 if num_of_review_set % BATCH_SIZE \u003e 0 else 0 )))\n","  loss_val_total = 0\n","  accuracy_val_total = 0\n","  temp_count = 0\n","  for i in t:\n","    val_start_i = i*BATCH_SIZE\n","    val_end_i = (i+1)*BATCH_SIZE\n","    # print(val_start_i, val_end_i, indices.shape)\n","    batch_val = format_reviews(xlnet_tokenizer, reviews_dataset, indices[val_start_i:val_end_i], review_sentiment_dict=review_sentiment_dict)\n","    (batch_input_val, batch_target_val, batch_review_sentiment_val, batch_target_mask_val) = batch_val\n","    # print(batch_input_val.shape, batch_review_sentiment_val.shape)\n","    (batch_input_val, batch_target_val) = list_to_device((batch_input_val, batch_target_val))\n","    batch_target_mask_val, batch_review_sentiment_val = list_to_device((batch_target_mask_val, batch_review_sentiment_val))\n","    # print(batch_input_val.shape, batch_review_sentiment_val.shape)\n","    prediction_val = model.forward(batch_input_val, batch_review_sentiment_val)\n","    # print(prediction_val.size(), batch_target_val.size())\n","    # print(prediction_val, batch_target_val)\n","    loss_val_total += model.loss_fn(prediction_val, batch_target_val).item()\n","    # print(loss_val)\n","    accuracy_val_total += torch.mean(torch.eq(prediction_val.argmax(dim=1,keepdim=False) + 1,batch_target_val).float()).item()\n","    temp_count += 1\n","    if i % round(8000 / BATCH_SIZE) == 0 and i != 0 and use_all:\n","      print(mode, \"Prelim Evaluation set loss:\", loss_val_total / temp_count, mode, \"Prelim Accuracy:\", accuracy_val_total / temp_count)\n","  loss_val = loss_val_total / temp_count\n","  accuracy_val = accuracy_val_total / temp_count\n","  print(mode, \"Evaluation set loss:\", loss_val, mode, \"Accuracy set %:\", accuracy_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rdf7J3q2deHw"},"outputs":[],"source":["losses = []\n","accuracies = []\n","\n","epoch_start = 0\n","t_start = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XtCwm9WBYwTh"},"outputs":[],"source":["# ONLY RUN THIS CELL (and next cell) if want to load checkpoint\n","# If you accidentally run this cell, no harm done (be careful with next cell!!!)\n","\n","checkpoint = None\n","try:\n","  checkpoint = torch.load(str(TORCH_CHECKPOINT_MODEL))\n","  print(\"Checkpoint loaded\")\n","except:\n","  print(\"No Checkpoint loaded\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"my4dZn8PiQ7Q"},"outputs":[],"source":["lr = 1e-4\n","optimizer_method = optim.Adam\n","optimizer = optimizer_method(model.parameters(), lr=lr)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IUgQA5xkcZnZ"},"outputs":[],"source":["# ONLY RUN THIS CELL if want to load checkpoint\n","\n","if checkpoint:\n","  epoch_start = checkpoint['epoch']\n","  t_start = checkpoint['t']\n","  model.load_state_dict(checkpoint['model_state_dict'])\n","  optimizer = optimizer_method(model.parameters(), lr=lr)\n","  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","  losses = checkpoint['losses']\n","  accuracies = checkpoint['accuracies']\n","  model.to(device)\n","\n","  print(\"Checkpoint\")\n","  run_validation(model)\n","\n","  print(f\"Checkpoint Epoch: {epoch_start} Iteration: {i} Loss: {np.mean(losses[-10:])} Accuracy: {np.mean(accuracies[-10:])}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oJqPzEHrc8iU"},"outputs":[],"source":["# set model to training mode\n","# Needs to be placed after the Checkpoint file loading\n","model.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uj1oMxxdyb_g"},"outputs":[],"source":["# https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n","\n","DATASET = train_reviews\n","\n","# Constants of interest: BATCH_SIZE, EPOCHS\n","\n","since = time.time()\n","\n","# Sanity check saving\n","torch.save({'epoch': 0,\n","            't': 0,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'losses': losses,\n","            'accuracies': accuracies\n","            }, str(TORCH_CHECKPOINT_MODEL) + \"FAKE\")\n","\n","# start training\n","for epoch in range(epoch_start, EPOCHS):\n","  indices = np.random.permutation(DATASET.shape[0])\n","\n","  dataset_batch_cap = ( DATASET.shape[0] // BATCH_SIZE ) + (1 if DATASET.shape[0] % BATCH_SIZE \u003e 0 else 0)\n","\n","  t = tqdm.notebook.tqdm(range(t_start, dataset_batch_cap), initial = t_start, total = dataset_batch_cap)\n","  \n","  for i in t:\n","    # batch\n","    batch = format_reviews(xlnet_tokenizer, DATASET, indices[i*BATCH_SIZE:(i+1)*BATCH_SIZE], review_sentiment_dict=review_sentiment_dict)\n","    (batch_input, batch_target, batch_review_sentiment, batch_target_mask) = batch\n","    # for item in (batch_input, batch_target, batch_review_sentiment, batch_target_mask):\n","    #   print(item.size())\n","    (batch_input, batch_target, batch_target_mask, batch_review_sentiment) = list_to_device((batch_input, batch_target, batch_target_mask, batch_review_sentiment))\n","    model.to(device)\n","    \n","    # forward pass\n","    prediction = model.forward(batch_input, batch_review_sentiment)\n","    # print(prediction.size(), batch_target.size())\n","    loss = model.loss_fn(prediction, batch_target)\n","    # print(loss)\n","    losses.append(loss.item())\n","    accuracy = torch.mean(torch.eq(prediction.argmax(dim=1,keepdim=False) + 1,batch_target).float())\n","    accuracies.append(accuracy.item())\n","    \n","    # backward pass\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","    \n","    # visuallize data\n","    if i % 1000 == 0 and i != t_start:\n","      torch.save({'epoch': epoch,\n","                  't': i,\n","                  'model_state_dict': model.state_dict(),\n","                  'optimizer_state_dict': optimizer.state_dict(),\n","                  'losses': losses,\n","                  'accuracies': accuracies\n","                  }, str(TORCH_CHECKPOINT_MODEL))\n","      run_validation(model)\n","      print(f\"Epoch: {epoch} Iteration: {i} Train Loss: {np.mean(losses[-10:])} Train Accuracy: {np.mean(accuracies[-10:])}\")\n","\n","  t_start = 0\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4YzK1KNAur-V"},"outputs":[],"source":["# Save the latest model\n","print(\"Saving latest model to\", str(TORCH_CHECKPOINT_MODEL))\n","torch.save({'epoch': EPOCHS,\n","            't': (DATASET.shape[0] // BATCH_SIZE)+1,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'losses': losses,\n","            'accuracies': accuracies\n","            }, str(TORCH_CHECKPOINT_MODEL))"]},{"cell_type":"markdown","metadata":{"id":"W0YY9el9Hkhs"},"source":["## evaluate model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_DL1R1fIHj-o"},"outputs":[],"source":["# set model to evaluation model\n","model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xs6LjX9Ft2uY"},"outputs":[],"source":["run_validation(model, True, \"val\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EKvR5mfjHrpO"},"outputs":[],"source":["run_validation(model, True, \"test\")"]},{"cell_type":"markdown","metadata":{"id":"7Bz_dXFsUNYL"},"source":["#Playground"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q7y8-KfuUHgu"},"outputs":[],"source":["# This is a cheap solution to stops any run all that reaches my Playground\n","hard_stop = input(\"Hard Stop here. Enter any key to allow passage.\")\n","\n","if len(hard_stop) == 0:\n","  raise Exception(\"Hard Stop\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-H6wxN8eUfkT"},"outputs":[],"source":["print(list(tokenize(STARTER[\"text\"]))[:3])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gCOB14uCU8pt"},"outputs":[],"source":["import urllib.request\n","import io\n","import sentencepiece as spm\n","\n","# https://github.com/google/sentencepiece/tree/master/python\n","\n","# Loads model from URL as iterator and stores the model to BytesIO.\n","model = io.BytesIO()\n","spm.SentencePieceTrainer.train(\n","      sentence_iterator=STARTER[\"text\"], model_writer=model, vocab_size=1000)\n","\n","# Serialize the model as file.\n","# with open('out.model', 'wb') as f:\n","#   f.write(model.getvalue())\n","\n","# Directly load the model from serialized model.\n","sp = spm.SentencePieceProcessor(model_proto=model.getvalue())\n","print(sp.encode('this is test'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KUD62TccVGEh"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"cs182-nlp-oscar-tallercnn-vadar.ipynb","version":""},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.2"}},"nbformat":4,"nbformat_minor":0}